The Word2Vec model is based on a simple neural network. The neural network generates word embeddings based on only one hidden layer. A research milestone was passed when Google Research published the model in 2013. The input layer of the neural network expects a one-hot vector. The one-hot vector is a BoW vector for one single word. This means that all indices of that vector are set to 0 except for the index of the word, which is analyzed. This index is set to 1.